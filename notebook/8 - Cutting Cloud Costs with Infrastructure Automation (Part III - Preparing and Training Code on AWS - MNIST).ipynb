{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cutting Cloud Costs with Infrastructure Automation (Part III - Preparing and Training Code on AWS - MNIST)\n",
    "## Review\n",
    "At this point, we have a working EC2 instance loaded with the AWS Deep Learning AMI, with the jupyter notebook tested and ready to go.\n",
    "\n",
    "<img src = \"https://media.giphy.com/media/v1HYJrnAMt6gM/giphy.gif\" width = \"500px\">\n",
    "\n",
    "The last step I want to take to automate this analysis as much as I can is to just get a notebook ready so I can just import it and run it. I'll start with the MNIST data set because I basically already have that code written and tested on my own laptop. All I need to do is to\n",
    "- Prepare a jupyter notebook\n",
    "- Host it on git\n",
    "- Fire up my EC2\n",
    "- Clone the project and code onto the EC2\n",
    "- Train on the EC2\n",
    "- View / save results\n",
    "- Terminate the EC2 and _**stop the billing**_\n",
    "\n",
    "Theoretically, this should take no longer than like 5 minutes to set up, and then however long it takes for the model to train (I'm hoping for like less than 30 minutes for 10 epochs, but I honestly have no clue what to expect). I'll use this as the notebook to clone and run on the EC2.\n",
    "\n",
    "## MNIST Model\n",
    "Please refer to post \\#3 for the fully documented and commentated code. To keep this post light, I'm just going to use the code from post \\#3. MNIST should be good example for automation too because it comes with a built in function that loads the data so we don't even have to worry too much about the ETL portion.\n",
    "\n",
    "### Cost\n",
    "I just wanted to preface this notebook first by noting how much this exercise will cost me. If things go well, my balance after will only be $0.20 more than it is right now. Current balance:\n",
    "\n",
    "<img src = \"https://s3.ca-central-1.amazonaws.com/2017edmfasatb/chi_lars_face_detection/images/43_aws_billing_before.png\" width = \"500px\">\n",
    "\n",
    "That exchange rate is killing me... but what can I really do?\n",
    "\n",
    "<img src = \"https://i.giphy.com/media/e3C4pNKkr9rji/giphy.webp\" width = \"400px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install tflearn\n",
    "import os\n",
    "os.system(\"pip install tflearn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TFlearn libraries\n",
    "import tflearn\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.estimator import regression\n",
    "import tflearn.datasets.mnist as mnist\n",
    "\n",
    "# General purpose libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract data from mnist.load_data()\n",
    "x, y, x_test, y_test = mnist.load_data(one_hot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reshape x\n",
    "x_reshaped = x.reshape([-1, 28, 28, 1])\n",
    "print 'x_reshaped has the shape {}'.format(x_reshaped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reshape x_test\n",
    "x_test_reshaped = x_test.reshape([-1, 28, 28, 1])\n",
    "print 'x_test_reshaped has the shape {}'.format(x_test_reshaped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sentdex's code to build the neural net using tflearn\n",
    "#   Input layer --> conv layer w/ max pooling --> conv layer w/ max pooling --> fully connected layer --> output layer\n",
    "convnet = input_data(shape = [None, 28, 28, 1], name = 'input')\n",
    "\n",
    "convnet = conv_2d(convnet, 32, 2, activation = 'relu')\n",
    "convnet = max_pool_2d(convnet, 2)\n",
    "\n",
    "convnet = conv_2d(convnet, 64, 2, activation = 'relu')\n",
    "convnet = max_pool_2d(convnet, 2)\n",
    "\n",
    "convnet = fully_connected(convnet, 1024, activation = 'relu')\n",
    "# convnet = dropout(convnet, 0.8)\n",
    "\n",
    "convnet = fully_connected(convnet, 10, activation = 'softmax')\n",
    "convnet = regression(convnet, optimizer = 'sgd', learning_rate = 0.01, loss = 'categorical_crossentropy', name = 'targets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = tflearn.DNN(convnet)\n",
    "model.fit(\n",
    "    {'input': x_reshaped}, \n",
    "    {'targets': y}, \n",
    "    n_epoch = 1, \n",
    "    validation_set = ({'input': x_test_reshaped}, {'targets': y_test}), \n",
    "    snapshot_step = 500, \n",
    "    show_metric = True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
