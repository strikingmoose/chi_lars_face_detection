{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12 - Building & Training Convolutional Neural Network\n",
    "## Preface\n",
    "Note that this same notebook crashed my laptop when I tried to train my CNN, so I'm migrating this onto AWS. This notebook's code is a similar copy of the previous notebook in this series except for this preface and the section after the model has been successfully trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Install tflearn\n",
    "import os\n",
    "os.system(\"sudo pip install tflearn tqdm boto3 opencv-python\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import urllib\n",
    "import math\n",
    "import boto3\n",
    "import os\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Connect to s3 bucket\n",
    "s3 = boto3.resource('s3', region_name = 'ca-central-1')\n",
    "my_bucket = s3.Bucket('2017edmfasatb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get all files in the project directory under chi_lars_face_detection/photos/\n",
    "chi_photos = [i.key for i in my_bucket.objects.all() if 'chi_lars_face_detection/photos/chi/' in i.key]\n",
    "lars_photos = [i.key for i in my_bucket.objects.all() if 'chi_lars_face_detection/photos/lars/' in i.key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define function to convert URL to numpy array\n",
    "def url_to_image(url):\n",
    "    # Download the image, convert it to a numpy array, and then read it into OpenCV format\n",
    "    resp = urllib.urlopen(url)\n",
    "    image = np.asarray(bytearray(resp.read()), dtype=\"uint8\")\n",
    "    image = cv2.imdecode(image, cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "    # Rotate image\n",
    "    image = np.rot90(image, 3)\n",
    "    \n",
    "    # Build resize into function\n",
    "    image = cv2.resize(image, (0,0), fx=0.03, fy=0.03)\n",
    "\n",
    "    # Return the image\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loop through all files to download into a single array from AWS\n",
    "url_prefix = 'https://s3.ca-central-1.amazonaws.com/2017edmfasatb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 203/203 [04:20<00:00,  1.36s/it]\n"
     ]
    }
   ],
   "source": [
    "# Trying out the new tqdm library for progress bar\n",
    "chi_photos_list = [url_to_image(os.path.join(url_prefix, x)) for x in tqdm(chi_photos)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [04:21<00:00,  1.22s/it]\n"
     ]
    }
   ],
   "source": [
    "# Trying out the new tqdm library for progress bar\n",
    "lars_photos_list = [url_to_image(os.path.join(url_prefix, x)) for x in tqdm(lars_photos)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert to numpy arrays\n",
    "chi_photos_np = np.array(chi_photos_list)\n",
    "lars_photos_np = np.array(lars_photos_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Temporarily save np arrays\n",
    "np.save('chi_photos_np_0.03_compress', chi_photos_np)\n",
    "np.save('lars_photos_np_0.03_compress', lars_photos_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Temporarily load from np arrays\n",
    "chi_photos_np = np.load('chi_photos_np_0.03_compress.npy')\n",
    "lars_photos_np = np.load('lars_photos_np_0.03_compress.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(203, 91, 91)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View shape of numpy array\n",
    "chi_photos_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set width var\n",
    "width = chi_photos_np.shape[-1]\n",
    "width"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Try out scaler on a manually set data (min of 0, max of 255)\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0],\n",
       "       [255]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set test data list to train on (min of 0, max of 255)\n",
    "test_list = np.array([0, 255]).reshape(-1, 1)\n",
    "test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize scaler\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chiwang/anaconda/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MinMaxScaler(copy=True, feature_range=(0, 1))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit test list\n",
    "scaler.fit(test_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshaping 3D Array To 4D Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(203, 91, 91, 1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chi_photos_np.reshape(-1, width, width, 1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting It All Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[135, 139, 139, ..., 210, 142, 136]], dtype=uint8)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reshape to prepare for scaler\n",
    "chi_photos_np_flat = chi_photos_np.reshape(1, -1)\n",
    "chi_photos_np_flat[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.52941176,  0.54509804,  0.54509804, ...,  0.82352941,\n",
       "         0.55686275,  0.53333333]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scale\n",
    "chi_photos_np_scaled = scaler.transform(chi_photos_np_flat)\n",
    "chi_photos_np_scaled[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reshape to prepare for scaler\n",
    "lars_photos_np_flat = lars_photos_np.reshape(1, -1)\n",
    "lars_photos_np_scaled = scaler.transform(lars_photos_np_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's reshape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chi_photos_reshaped has shape: (203, 91, 91, 1)\n",
      "lars_photos_reshaped has shape: (200, 91, 91, 1)\n"
     ]
    }
   ],
   "source": [
    "# Reshape\n",
    "chi_photos_reshaped = chi_photos_np_scaled.reshape(-1, width, width, 1)\n",
    "lars_photos_reshaped = lars_photos_np_scaled.reshape(-1, width, width, 1)\n",
    "\n",
    "print('{} has shape: {}'. format('chi_photos_reshaped', chi_photos_reshaped.shape))\n",
    "print('{} has shape: {}'. format('lars_photos_reshaped', lars_photos_reshaped.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_input has shape: (203, 91, 91, 1)\n"
     ]
    }
   ],
   "source": [
    "# Create copy of chi's photos to start populating x_input\n",
    "x_input = copy.deepcopy(chi_photos_reshaped)\n",
    "\n",
    "print('{} has shape: {}'. format('x_input', x_input.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_input has shape: (403, 91, 91, 1)\n"
     ]
    }
   ],
   "source": [
    "# Concatentate lars' photos to existing x_input\n",
    "x_input = np.append(x_input, lars_photos_reshaped, axis = 0)\n",
    "\n",
    "print('{} has shape: {}'. format('x_input', x_input.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_chi has shape: (203, 2)\n",
      "y_lars has shape: (200, 2)\n"
     ]
    }
   ],
   "source": [
    "# Create label arrays\n",
    "y_chi = np.array([[1, 0] for i in chi_photos_reshaped])\n",
    "y_lars = np.array([[0, 1] for i in lars_photos_reshaped])\n",
    "\n",
    "print('{} has shape: {}'. format('y_chi', y_chi.shape))\n",
    "print('{} has shape: {}'. format('y_lars', y_lars.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview the first few elements\n",
    "y_chi[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_lars[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_input has shape: (203, 2)\n"
     ]
    }
   ],
   "source": [
    "# Create copy of chi's labels to start populating y_input\n",
    "y_input = copy.deepcopy(y_chi)\n",
    "\n",
    "print('{} has shape: {}'. format('y_input', y_input.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_input has shape: (403, 2)\n"
     ]
    }
   ],
   "source": [
    "# Concatentate lars' labels to existing y_input\n",
    "y_input = np.append(y_input, y_lars, axis = 0)\n",
    "\n",
    "print('{} has shape: {}'. format('y_input', y_input.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "I'm going to just copy and paste the CNN structure I used for the MNIST tutorial and see what happens. I'm running this on my own laptop by the way, let's observe the speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TFlearn libraries\n",
    "import tflearn\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.estimator import regression\n",
    "import tflearn.datasets.mnist as mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sentdex's code to build the neural net using tflearn\n",
    "#   Input layer --> conv layer w/ max pooling --> conv layer w/ max pooling --> fully connected layer --> output layer\n",
    "convnet = input_data(shape = [None, width, width, 1], name = 'input')\n",
    "\n",
    "convnet = conv_2d(convnet, 32, 10, activation = 'relu')\n",
    "convnet = max_pool_2d(convnet, 2)\n",
    "\n",
    "convnet = conv_2d(convnet, 64, 10, activation = 'relu')\n",
    "convnet = max_pool_2d(convnet, 2)\n",
    "\n",
    "convnet = fully_connected(convnet, 1024, activation = 'relu')\n",
    "convnet = dropout(convnet, 0.8)\n",
    "\n",
    "convnet = fully_connected(convnet, 2, activation = 'softmax')\n",
    "convnet = regression(convnet, optimizer = 'sgd', learning_rate = 0.01, loss = 'categorical_crossentropy', name = 'targets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split\n",
    "I'm just going to do a 90 / 10 train test split here\n",
    "- My training data will consist of roughly 360 training images\n",
    "- My test data will consist of roughly 40 test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import library\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(403, 91, 91, 1)\n",
      "(403, 2)\n"
     ]
    }
   ],
   "source": [
    "print(x_input.shape)\n",
    "print(y_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Perform train test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_input, y_input, test_size = 0.1, stratify = y_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "Let's try training with 3 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 35  | total loss: \u001b[1m\u001b[32m0.01546\u001b[0m\u001b[0m | time: 123.529s\n",
      "| SGD | epoch: 003 | loss: 0.01546 - acc: 0.9999 -- iter: 704/724\n",
      "Training Step: 36  | total loss: \u001b[1m\u001b[32m0.01470\u001b[0m\u001b[0m | time: 141.459s\n",
      "| SGD | epoch: 003 | loss: 0.01470 - acc: 0.9999 | val_loss: 0.00820 - val_acc: 1.0000 -- iter: 724/724\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "# Train with data\n",
    "model = tflearn.DNN(convnet)\n",
    "model.fit(\n",
    "    {'input': x_train},\n",
    "    {'targets': y_train},\n",
    "    n_epoch = 3,\n",
    "    validation_set = ({'input': x_test}, {'targets': y_test}),\n",
    "    snapshot_step = 500,\n",
    "    show_metric = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:/Users/chiwang/Documents/Projects/Dev/chi_lars_face_detection/notebook/model_4_epochs_0.03_compression_99.6.tflearn is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    }
   ],
   "source": [
    "# Save model\n",
    "model.save('model_4_epochs_0.03_compression_99.6.tflearn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "Okay, so that was quite the wild ride. I've gotten something to work right now and it's giving me _**99.99% accuracy**_. (0.0147 loss). The loss is cross entropy (measuring node purity, something to the tune of $D=-\\sum_{k=1}^{K}\\hat{p}_{mk}log{\\hat{p}_{mk}}$), so that seems like quite a good loss value to have. Let's try to predict on our test set and generate a simple confusion matrix just to make sure we're sane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.0052014 ,  0.99479854],\n",
       "       [ 0.00778673,  0.99221331],\n",
       "       [ 0.00471839,  0.99528164],\n",
       "       [ 0.00564974,  0.99435025],\n",
       "       [ 0.00522123,  0.99477875],\n",
       "       [ 0.00666202,  0.99333799],\n",
       "       [ 0.00466673,  0.99533325],\n",
       "       [ 0.00777068,  0.99222928],\n",
       "       [ 0.98755813,  0.01244185],\n",
       "       [ 0.99169731,  0.00830262]], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict on test set, generating probabilities of each class (one-hot style)\n",
    "y_pred_proba = model.predict(x_test)\n",
    "y_pred_proba[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['lars', 'lars', 'lars', 'lars', 'lars', 'lars', 'lars', 'lars',\n",
       "       'chi', 'chi'], \n",
       "      dtype='|S4')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert probabilities to direct predictions\n",
    "y_pred_labels = np.array(['chi' if y[0] >= 0.5 else 'lars' for y in y_pred_proba])\n",
    "y_pred_labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['lars', 'lars', 'lars', 'lars', 'lars', 'lars', 'lars', 'lars',\n",
       "       'chi', 'chi'], \n",
       "      dtype='|S4')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert y_test to direct predictions\n",
    "y_test_labels = np.array(['chi' if y[0] >= 0.5 else 'lars' for y in y_test])\n",
    "y_test_labels[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like it's gotten the first 10 right so far... _**creepy**_. Let's make a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[35,  0],\n",
       "       [ 0, 47]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test_labels, y_pred_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, there's that 100%... It guessed 35 / 35 of my photos and 47 / 47 of my girlfriend's photos... Amazing? I really don't know how to answer this question... I don't know enough yet to answer this question yet.\n",
    "\n",
    "On one hand...\n",
    "\n",
    "<span style=\"color:red; font-size: 3em;\">**DID IT JUST LITERALLY TELL THE DIFFERENCE BETWEEN TWO PEOPLE? ALL THAT TOOK WAS LIKE 10 LINES OF TFLEARN CODE?!?!?!**</span>\n",
    "\n",
    "I feel like the neural net can understand not only my face, but my life and my soul... I'm a bit scared, but it's what I set out to do so I'm obviously very happy.\n",
    "\n",
    "On the other hand, an experienced practitioner can probably scoff and laugh at how controlled my experiment was...\n",
    "- I had a small training sample size\n",
    "- I had a small test sample size\n",
    "- I had the same background\n",
    "- We wore the same clothes\n",
    "- Accessories and hairstyles were all the same\n",
    "- Only faces changed, but face position or head position relative to the photo frame\n",
    "\n",
    "We'd probably want to test this with a few more photos with varying factors to understand a bit better how our NN operates. I'll continue with this in the next post, but before that I want to talk about some very important details that I have glossed over.\n",
    "\n",
    "## Looping Back To The Details\n",
    "There are many things I want to talk about because it took me basically an entire evening to actually get that thing to train.\n",
    "\n",
    "The biggest problem I had was getting an \"OOM\" or _**Out-Of-Memory**_ error. This took me so long to figure out, but the issue was that I was using way too many filters in my convolutional layers. Before, I had decided on using 100 filters in my first layer and 200 filters in my second layer. My logic here was clearly flawed, but was based the fact that we had 32 filters for our 28 x 28 MNIST data set, so I thought I should use a number of filters that was similar to the dimensions of my photo. This immediately caused an OOM error.\n",
    "\n",
    "The second problem was that I'm not quite understanding how the variables and objects are built and stored within TFlearn. After I ran that model that yielded the OOM error, I tried re-tweaking the parameters, and I kept getting that error. I tried many different things, tweaking the number / size of convolutional filters, the number of nodes in the fully connected layer, the input size of the image (I thought that maybe ~100 x 100 pixels was too large of an input, so I scaled the image even smaller to ~25 x 25 like MNIST), but I kept getting the OOM error. Eventually, I figured out that I needed to _**reset the kernel in the jupyter notebook**_ because everytime I built my NN, I was building on top of the NN I had already built, just adding onto the first 100 filter convolutional layer. It turns out, at the end, that the 100 and 200 filters was what was causing the problems.\n",
    "\n",
    "I also want to touch really quickly on the number of epochs as well. TFlearn gives us the benefit of seeing how the training process is progressing through the epochs. Technically, I would use cross validation to find the optimal number of epochs (not to mention all the other NN parameters as well), but I haven't quite figured out / explored how to use GridSearchCV with TFlearn yet. I just watched the error metrics while the epochs trained and 3 seemed to do the best as, at one point, it reached 100% accuracy (result still yielded 100% accuracy).\n",
    "\n",
    "At the end of the day, after I figured out the memory issues, I actually just ended up training the NN on my own laptop. At such a small sample size and only 3 epochs, the process took around 6 minutes. No big deal and I can save myself \\$0.20 in the process :).\n",
    "\n",
    "That's all for now. Let's test with a more complex test set in the next post!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
