{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cutting Cloud Costs with Infrastructure Automation (Part IV - Preparing and Training Code on AWS w/ Drop Out - MNIST)\n",
    "I said in the last post that I was ready to try this on our face. I'm totally not ready - I'm just missing one last thing I wanted to try. TFlearn gives us the option of easily including _**drop out**_ in our fully connected layers of the NN, and I wanted to explore that really quickly before moving on because I've read about it and it seems super easily to implement.\n",
    "\n",
    "## Drop Out\n",
    "Drop out is a simple regularization concept in NNs. When we implement drop out, we're telling the NN to basically pretend like a few of the neurons in the fully connected layer don't exist. This forces the other neurons (who have not been dropped out) to remodel their weights / thresholds to work with different neurons in every iteration of training.\n",
    "\n",
    "![](http://cs231n.github.io/assets/nn2/dropout.jpeg)\n",
    "\n",
    "I like to think of this method employing the same logic that subsampling does in tree techniques like random forests. When we subsample in random forest, we choose only a subset of samples and / or features to train on. This basically forces different features to explicitly coexist with some features and prevent the coexistence with other features to get a bit more perspective on which features are truly more important.\n",
    "\n",
    "Dropping out within NNs also force features (or linear combinations of features, depending on how deep we get into our fully connected layers) to work nice with other features to see which really end up making an impact in every situation before completing our model training.\n",
    "\n",
    "Dropping out in TFlearn is super simple. There's literally a _**dropout**_ function where you tell it what percentage of the neurons to keep in every iteration and TFlearn will perform this for you by dropping our random neurons.\n",
    "\n",
    "It's actually been in the code this whole time because sentdex used dropout in his model and I just copied sentdex, but I've been commenting it out. All the code below is _**the exact same as the previous post**_ except the dropout line when building our NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Install tflearn\n",
    "import os\n",
    "os.system(\"sudo pip install tflearn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TFlearn libraries\n",
    "import tflearn\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.estimator import regression\n",
    "import tflearn.datasets.mnist as mnist\n",
    "\n",
    "# General purpose libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist/train-images-idx3-ubyte.gz\n",
      "Extracting mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Extract data from mnist.load_data()\n",
    "x, y, x_test, y_test = mnist.load_data(one_hot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_reshaped has the shape (55000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "# Reshape x\n",
    "x_reshaped = x.reshape([-1, 28, 28, 1])\n",
    "print 'x_reshaped has the shape {}'.format(x_reshaped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_test_reshaped has the shape (10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "# Reshape x_test\n",
    "x_test_reshaped = x_test.reshape([-1, 28, 28, 1])\n",
    "print 'x_test_reshaped has the shape {}'.format(x_test_reshaped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sentdex's code to build the neural net using tflearn\n",
    "#   Input layer --> conv layer w/ max pooling --> conv layer w/ max pooling --> fully connected layer --> output layer\n",
    "convnet = input_data(shape = [None, 28, 28, 1], name = 'input')\n",
    "\n",
    "convnet = conv_2d(convnet, 32, 2, activation = 'relu')\n",
    "convnet = max_pool_2d(convnet, 2)\n",
    "\n",
    "convnet = conv_2d(convnet, 64, 2, activation = 'relu')\n",
    "convnet = max_pool_2d(convnet, 2)\n",
    "\n",
    "convnet = fully_connected(convnet, 1024, activation = 'relu')\n",
    "convnet = dropout(convnet, 0.8)\n",
    "\n",
    "convnet = fully_connected(convnet, 10, activation = 'softmax')\n",
    "convnet = regression(convnet, optimizer = 'sgd', learning_rate = 0.01, loss = 'categorical_crossentropy', name = 'targets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 4299  | total loss: \u001b[1m\u001b[32m0.11463\u001b[0m\u001b[0m | time: 11.579s\n",
      "| SGD | epoch: 005 | loss: 0.11463 - acc: 0.9676 -- iter: 54976/55000\n",
      "Training Step: 4300  | total loss: \u001b[1m\u001b[32m0.11163\u001b[0m\u001b[0m | time: 12.897s\n",
      "| SGD | epoch: 005 | loss: 0.11163 - acc: 0.9693 | val_loss: 0.09128 - val_acc: 0.9733 -- iter: 55000/55000\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "model = tflearn.DNN(convnet)\n",
    "model.fit(\n",
    "    {'input': x_reshaped}, \n",
    "    {'targets': y}, \n",
    "    n_epoch = 5, \n",
    "    validation_set = ({'input': x_test_reshaped}, {'targets': y_test}), \n",
    "    snapshot_step = 500, \n",
    "    show_metric = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "Nice. We're at 97.3% and we've gained like 0.7%, which is actually a big deal in this application! At this level, we're basically trying to squeeze every 1/10th of a percent we can get haha."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
